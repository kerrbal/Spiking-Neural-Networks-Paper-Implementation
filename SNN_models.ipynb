{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b06e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.autograd import Function\n",
    "from torch.nn import functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c8e56b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spike_fn(U, threshold):\n",
    "    return (U >= threshold).float()\n",
    "\n",
    "\n",
    "#input X spikes is determined by the probability,\n",
    "#higher the pixel tensity, higher the probability there is a spike.\n",
    "def poisson_encode(x, T):\n",
    "    x = x.unsqueeze(0).repeat(T, 1, 1)  # (T, B, features)\n",
    "    spikes = (torch.rand_like(x) < x).float()\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55ddb5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, tau_trace = 20.0,\n",
    "                 tau_mem=20.0, tau_syn=5.0, dt=1.0, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "\n",
    "        # use Xavier initialization, because otherwise some neurons were not firing at all, it was unstable.\n",
    "        std = 1.0 / (in_features**0.5)\n",
    "        self.W = nn.Parameter(torch.randn(out_features, in_features) * std)\n",
    "\n",
    "\n",
    "        self.dt = dt #just a theoretical constant\n",
    "\n",
    "        #Use register_buffer, because it must be linked to the nn.Module\n",
    "        self.register_buffer('alpha', torch.exp(torch.tensor(-dt / tau_syn))) #Voltage decay\n",
    "        self.register_buffer('beta', torch.exp(torch.tensor(-dt / tau_mem))) #Current decay\n",
    "        self.register_buffer('gamma', torch.exp(torch.tensor(-dt / tau_trace)))  #Trace decay\n",
    "\n",
    "\n",
    "    #input spikes -> (T, B, in_dim)\n",
    "    #returns -> (T, B, out_dim) as spikes, and (T, B, out_dim) as voltage values\n",
    "    def forward(self, input_spikes):\n",
    "\n",
    "        T, B, _ = input_spikes.shape\n",
    "        device = input_spikes.device\n",
    "\n",
    "        #current, and voltage must start from 0 in the beginning\n",
    "        I = torch.zeros(B, self.out_features, device=device)\n",
    "        U = torch.zeros(B, self.out_features, device=device)\n",
    "\n",
    "        #keep all spike and voltage values across the time steps.\n",
    "        out_spikes = []\n",
    "        U_hist = []\n",
    "\n",
    "        for t in range(T):\n",
    "            x_t = input_spikes[t]  # (B, in_features), X value of the current time step.\n",
    "\n",
    "            #new current\n",
    "            I = self.alpha * I + x_t @ self.W.t()  # (B, out_features)\n",
    "\n",
    "            #new membrane voltage\n",
    "            U = self.beta * U + I\n",
    "\n",
    "            #if U exceeds threshold get one as a spike.\n",
    "            S = spike_fn(U, self.threshold)  # (B, out_features)\n",
    "            U_hist.append(U.clone())\n",
    "            U = U - S * self.threshold #reset U\n",
    "\n",
    "            out_spikes.append(S)\n",
    "            \n",
    "\n",
    "        out_spikes = torch.stack(out_spikes, dim=0)\n",
    "        U_hist = torch.stack(U_hist, dim=0)\n",
    "        return out_spikes, U_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f200c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RandomBPSNN(nn.Module):\n",
    "    def __init__(self, T=20, tau_mem=20.0, tau_syn=5.0, dt=1.0, threshold=1.0):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.lif_layers = torch.nn.ModuleList()\n",
    "        self.G_hiddens = torch.nn.ParameterList()\n",
    "\n",
    "\n",
    "    def append_LIF(self, LIF, out_dim = 10):\n",
    "        self.lif_layers.append(LIF)\n",
    "        if (len(self.lif_layers) > 1):\n",
    "            hidden_dim = self.lif_layers[-2].out_features #set the projection matrix\n",
    "            #random projection matrix -> (out_dim(10), hidden_dim)\n",
    "            G_hidden = nn.Parameter(\n",
    "                torch.randn(out_dim, hidden_dim) * 0.1, requires_grad = False\n",
    "            )\n",
    "            self.G_hiddens.append(G_hidden)\n",
    "\n",
    "    #here if X is a static no temporal data -> X values are encoded using rate-encoding or poisson encoding\n",
    "    #generally rate-encoding is used because it is more stable however I implemented poisson encoding above as well.\n",
    "    #if it has a stime steps, no encoding is used.\n",
    "\n",
    "    #Static X -> (B, in_dim)\n",
    "    #Time_step_X -> (T, B, in_dim)\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        static = len(x.shape) == 2\n",
    "        if (static):\n",
    "            B, in_dim = x.shape\n",
    "            x = x.unsqueeze(0).repeat(self.T, 1, 1)  # (T, B, in_dim), rate encoding\n",
    "\n",
    "        else:\n",
    "            _, B, in_dim = x.shape\n",
    "        o_spk = x\n",
    "        spikes, voltages = [], []\n",
    "        for lif in self.lif_layers:\n",
    "            o_spk, o_U = lif(o_spk)\n",
    "            spikes.append(o_spk)\n",
    "            voltages.append(o_U)\n",
    "\n",
    "        #to calculate accuracy, we generally sum spike numbers of all the time steps, and predict the \n",
    "        #class which has maximum number of summed spikies.\n",
    "        out_rate = o_spk.mean(dim=0)  #(B, O)\n",
    "\n",
    "        return {\n",
    "            \"Us\": voltages,\n",
    "            \"spikes\": spikes,\n",
    "            \"o_spk\": spikes[-1],\n",
    "            \"o_U\": voltages[-1],\n",
    "            \"out_rate\": out_rate\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60b7aff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_bp_step(model, x, target, optimizer, super_spike_B = 25, last_learning_window = 5):\n",
    "    \"\"\"\n",
    "    model -> all_layers(RandomBPSNN)\n",
    "    x -> (B, in_dim) or (T, B, in_dim) if it is (B, in_dim) timesteps are produced using rate-encoding\n",
    "    target -> (B, out_dim)\n",
    "    loss_fn -> Cross entropy derivative is used here -> |Y - y_predicted|\n",
    "    last_learning_window -> last last_learnin_window number is used for determining loss and derivatives.\n",
    "    increase it -> it pay attention to more time steps, since the loss signal is stronger, the latency is shorter\n",
    "    since it might wait to learn\n",
    "    decreasing it -> network only pays attention to last spikes, the latency is longer because it waits for the last spikes\n",
    "    \"\"\"    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    out = model(x)\n",
    "    out_spikes = out[\"o_spk\"] # (T, B, 10)\n",
    "    T, B, num_classes = out_spikes.shape\n",
    "    \n",
    "    # DEBUG: Spike rate kontrol\n",
    "    #print(f\"Spike rate: {out_spikes.mean().item():.4f}\")\n",
    "    #print(f\"Output spike sum per class: {out_spikes.sum(dim=0).mean(dim=0)}\")\n",
    "    #apply one_hot encoding if it hasnt applied yet.\n",
    "    if (len(target.shape) == 1):\n",
    "        target = F.one_hot(target, num_classes=num_classes).float()  # (B, 10)\n",
    "\n",
    "\n",
    "    #Surrogate derivative -> SuperSpike -> 1 / (1 + B*|U - threshold|)^2\n",
    "    #calculate surrogate gradients\n",
    "    o_U = out[\"o_U\"]  #(T, B, O)\n",
    "    T = o_U.shape[0]\n",
    "\n",
    "    #dL/dy^(L) = (out_rate - target)\n",
    "    learning_window = out_spikes[T-last_learning_window:].mean(dim = 0, keepdim = False) #(B, O)\n",
    "    dL_dyL = learning_window - target  # (B, O)\n",
    "    #probs = torch.softmax(learning_window) #Calculate softmax(probabilities)\n",
    "    loss = F.cross_entropy(learning_window, target) #calculate cross_entropy loss function\n",
    "    #print(f\"Learning window min: {learning_window.min().item():.4f}\")\n",
    "    #print(f\"Learning window max: {learning_window.max().item():.4f}\")\n",
    "    #print(f\"Loss: {loss.item():.4f}\")\n",
    "    super_spike_B = super_spike_B\n",
    "    du = (o_U - model.lif_layers[-1].threshold).abs() #(T, B, O)\n",
    "    sigma_prime_per_t = 1 / ((1 + super_spike_B * du)**2) # (T, B, O)\n",
    "    \n",
    "    #(B, O) - average over time\n",
    "    sigma_prime_out = sigma_prime_per_t[T-last_learning_window:, :, :].mean(dim=0, keepdim = False)\n",
    "    #take derivative for the output layer:\n",
    "    input_of_out_layer = out[\"spikes\"][-2] # (T, B, hidden_units)\n",
    "    learning_window_hiddens = input_of_out_layer[T-last_learning_window:].mean(dim = 0, keepdim = False)#(B, hidden_units)\n",
    "\n",
    "    delta_out = dL_dyL * sigma_prime_out  # (B, O)\n",
    "\n",
    "    dw_output = torch.permute(delta_out, (1, 0)) @ learning_window_hiddens #(O, hidden_units)\n",
    "    dw_output = dw_output / B #(O, hidden_units)\n",
    "    model.lif_layers[-1].W.grad = dw_output #set the derivative\n",
    " \n",
    "    voltages = out[\"Us\"]\n",
    "    spikes = out[\"spikes\"]\n",
    "    for i in range(len(voltages) - 1):\n",
    "        voltage = voltages[i] # (T, B, H)\n",
    "        \n",
    "        layer_threshold = model.lif_layers[i].threshold\n",
    "        du = (voltage[T-last_learning_window:, :, :] - layer_threshold).abs()\n",
    "        sigma_prime_per_t = 1 / ((1 + super_spike_B * du)**2) # (T, B, O)\n",
    "        sigma_prime_out = sigma_prime_per_t[T-last_learning_window:, :, :].mean(dim=0, keepdim = False) #(B, hidden_dim)\n",
    "        if (i == 0):\n",
    "            input_spikes = x[T-last_learning_window:, :, :].mean(dim = 0, keepdim = False)#(B, hidden_dim)\n",
    "        else:\n",
    "            spike = spikes[i - 1] # (T, B, H)\n",
    "            input_spikes = spike[T-last_learning_window:, :, :].mean(dim = 0, keepdim = False)#(B, hidden_dim)\n",
    "        #calculate loss on the projected vector (for it being biologically plausible, it is more localized)\n",
    "        random_matrix = model.G_hiddens[i] # (output_dim, hidden_dim)\n",
    "        delta_hidden = sigma_prime_out*(delta_out @ random_matrix) # (B, O) @ (O, hidden_dim) -> (B, hidden_dim)\n",
    "        dw = torch.zeros_like(model.lif_layers[i].W)\n",
    "\n",
    "        dw += torch.permute(delta_hidden, (1, 0)) @ input_spikes #(O, B) @ (B, input_dim) -> (O, input_dim)\n",
    "        dw = dw/B #batch mean\n",
    "        #set gradients\n",
    "        model.lif_layers[i].W.grad = dw\n",
    "\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d1d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MNIST yÃ¼kleme\n",
    "transform = transforms.Compose([\n",
    "transforms.ToTensor()                # (1, 28, 28), [0,1]\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(root=\"../data\", train=True,\n",
    "                        download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root=\"../data\", train=False,\n",
    "                        download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04596790",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(train_loader, test_loader, model, optimizer, num_epochs, verbose = True, super_spike_beta = 25):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    spike_means_outer = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)   # (B,1,28,28)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # Flatten: (B, 784)\n",
    "            data = data.view(data.size(0), -1)\n",
    "            data = data.unsqueeze(0).repeat(20, 1, 1)  # (T, B, in_dim), rate encoding\n",
    "\n",
    "            total_loss += random_bp_step(model, data, target, optimizer, last_learning_window=15, super_spike_B=super_spike_beta)\n",
    "\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                if (verbose):\n",
    "                    print(f\"Epoch {epoch} | Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                        f\"Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "                data, target = next(iter(train_loader))\n",
    "                data = data.to(device).view(data.size(0), -1)\n",
    "\n",
    "                out = model(data)\n",
    "                spike_means = []\n",
    "                for i in out[\"spikes\"]:\n",
    "                    spike_means.append(i.mean().item())\n",
    "                spike_means_outer.append(torch.tensor(spike_means))\n",
    "                if (verbose):\n",
    "                    for i in range(len(spike_means)):\n",
    "                        print(f\"layer {i+1} spike means:\", spike_means[i])\n",
    "\n",
    "\n",
    "        # ---- Test accuracy ----\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        if (verbose):\n",
    "            print(f\"Epoch {epoch} finished. Test accuracy: {acc:.2f}%\")\n",
    "    spike_means_outer = torch.stack(spike_means_outer, dim = 0)\n",
    "    return model, acc, spike_means_outer\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            data = data.view(data.size(0), -1)\n",
    "\n",
    "            out = model(data)\n",
    "            out_rate = out[\"out_rate\"]  # (B, 10)\n",
    "            pred = out_rate.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2fc8339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256, 512]\n",
    "# Model\n",
    "device = torch.device(\"cuda\")\n",
    "def get_model(hidden_dims, thresholds = None, super_spike_beta = 25, lr = 1e-3):\n",
    "    if (thresholds is None):\n",
    "        thresholds = [1 for _ in range(len(hidden_dims) - 1)]\n",
    "\n",
    "    model = RandomBPSNN()\n",
    "    weights = []\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "        model.append_LIF(LIFLayer(hidden_dims[i-1], hidden_dims[i], threshold=thresholds[i-1]))\n",
    "        weights.append(model.lif_layers[-1].W)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        weights, lr=lr\n",
    "    )\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c767ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 | Batch 100/938 | Loss: 2.2950\n",
      "layer 1 spike means: 0.488525390625\n",
      "layer 2 spike means: 0.010859374888241291\n",
      "Epoch 1 | Batch 200/938 | Loss: 2.2834\n",
      "layer 1 spike means: 0.47679445147514343\n",
      "layer 2 spike means: 0.011015624739229679\n",
      "Epoch 1 | Batch 300/938 | Loss: 2.2640\n",
      "layer 1 spike means: 0.4914794862270355\n",
      "layer 2 spike means: 0.016640624031424522\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.2532\n",
      "layer 1 spike means: 0.4742065370082855\n",
      "layer 2 spike means: 0.014609375037252903\n",
      "Epoch 1 | Batch 500/938 | Loss: 2.2392\n",
      "layer 1 spike means: 0.48432618379592896\n",
      "layer 2 spike means: 0.025390625\n",
      "Epoch 1 | Batch 600/938 | Loss: 2.2278\n",
      "layer 1 spike means: 0.508544921875\n",
      "layer 2 spike means: 0.025156250223517418\n",
      "Epoch 1 | Batch 700/938 | Loss: 2.2129\n",
      "layer 1 spike means: 0.504626452922821\n",
      "layer 2 spike means: 0.03890625014901161\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.1943\n",
      "layer 1 spike means: 0.5222534537315369\n",
      "layer 2 spike means: 0.03218749910593033\n",
      "Epoch 1 | Batch 900/938 | Loss: 2.1767\n",
      "layer 1 spike means: 0.5126953125\n",
      "layer 2 spike means: 0.05039062350988388\n",
      "Epoch 1 finished. Test accuracy: 52.17%\n",
      "Epoch 2 | Batch 100/938 | Loss: 1.9868\n",
      "layer 1 spike means: 0.522570788860321\n",
      "layer 2 spike means: 0.049296874552965164\n",
      "Epoch 2 | Batch 200/938 | Loss: 1.9419\n",
      "layer 1 spike means: 0.4958252012729645\n",
      "layer 2 spike means: 0.05078125\n",
      "Epoch 2 | Batch 300/938 | Loss: 1.9233\n",
      "layer 1 spike means: 0.49822998046875\n",
      "layer 2 spike means: 0.06890624761581421\n",
      "Epoch 2 | Batch 400/938 | Loss: 1.9020\n",
      "layer 1 spike means: 0.529980480670929\n",
      "layer 2 spike means: 0.047187499701976776\n",
      "Epoch 2 | Batch 500/938 | Loss: 1.8823\n",
      "layer 1 spike means: 0.575793445110321\n",
      "layer 2 spike means: 0.07289062440395355\n",
      "Epoch 2 | Batch 600/938 | Loss: 1.8658\n",
      "layer 1 spike means: 0.5710815787315369\n",
      "layer 2 spike means: 0.07054687291383743\n",
      "Epoch 2 | Batch 700/938 | Loss: 1.8546\n",
      "layer 1 spike means: 0.5595703125\n",
      "layer 2 spike means: 0.05562499910593033\n",
      "Epoch 2 | Batch 800/938 | Loss: 1.8425\n",
      "layer 1 spike means: 0.552624523639679\n",
      "layer 2 spike means: 0.08632812649011612\n",
      "Epoch 2 | Batch 900/938 | Loss: 1.8298\n",
      "layer 1 spike means: 0.55096435546875\n",
      "layer 2 spike means: 0.08484374731779099\n",
      "Epoch 2 finished. Test accuracy: 73.27%\n",
      "Epoch 3 | Batch 100/938 | Loss: 1.7137\n",
      "layer 1 spike means: 0.5416259765625\n",
      "layer 2 spike means: 0.08703124523162842\n",
      "Epoch 3 | Batch 200/938 | Loss: 1.6834\n",
      "layer 1 spike means: 0.530163586139679\n",
      "layer 2 spike means: 0.09546874463558197\n",
      "Epoch 3 | Batch 300/938 | Loss: 1.6720\n",
      "layer 1 spike means: 0.525439441204071\n",
      "layer 2 spike means: 0.08531250059604645\n",
      "Epoch 3 | Batch 400/938 | Loss: 1.6620\n",
      "layer 1 spike means: 0.5179077386856079\n",
      "layer 2 spike means: 0.09234374761581421\n",
      "Epoch 3 | Batch 500/938 | Loss: 1.6548\n",
      "layer 1 spike means: 0.518505871295929\n",
      "layer 2 spike means: 0.08882812410593033\n",
      "Epoch 3 | Batch 600/938 | Loss: 1.6475\n",
      "layer 1 spike means: 0.5323242545127869\n",
      "layer 2 spike means: 0.0989062488079071\n",
      "Epoch 3 | Batch 700/938 | Loss: 1.6437\n",
      "layer 1 spike means: 0.5144287347793579\n",
      "layer 2 spike means: 0.09140624850988388\n",
      "Epoch 3 | Batch 800/938 | Loss: 1.6391\n",
      "layer 1 spike means: 0.5234741568565369\n",
      "layer 2 spike means: 0.08867187052965164\n",
      "Epoch 3 | Batch 900/938 | Loss: 1.6353\n",
      "layer 1 spike means: 0.51348876953125\n",
      "layer 2 spike means: 0.08796874433755875\n",
      "Epoch 3 finished. Test accuracy: 84.23%\n",
      "For hidden size 64 accuracy is -->  84.23\n",
      "Layer 0 W mean: 0.000094\n",
      "Layer 0 W std: 0.059923\n",
      "Layer 0 grad mean: 0.000001\n",
      "Layer 0 grad max: 0.000667\n",
      "Layer 1 W mean: -0.019628\n",
      "Layer 1 W std: 0.107357\n",
      "Layer 1 grad mean: 0.000215\n",
      "Layer 1 grad max: 0.012623\n",
      "Using device: cuda\n",
      "Epoch 1 | Batch 100/938 | Loss: 2.3092\n",
      "layer 1 spike means: 0.495614618062973\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 200/938 | Loss: 2.3059\n",
      "layer 1 spike means: 0.504595935344696\n",
      "layer 2 spike means: 0.00023437499476131052\n",
      "Epoch 1 | Batch 300/938 | Loss: 2.2738\n",
      "layer 1 spike means: 0.49191591143608093\n",
      "layer 2 spike means: 0.024843748658895493\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.2309\n",
      "layer 1 spike means: 0.49517822265625\n",
      "layer 2 spike means: 0.02617187425494194\n",
      "Epoch 1 | Batch 500/938 | Loss: 2.1880\n",
      "layer 1 spike means: 0.5031707882881165\n",
      "layer 2 spike means: 0.04859374836087227\n",
      "Epoch 1 | Batch 600/938 | Loss: 2.1483\n",
      "layer 1 spike means: 0.48330384492874146\n",
      "layer 2 spike means: 0.05445312336087227\n",
      "Epoch 1 | Batch 700/938 | Loss: 2.1119\n",
      "layer 1 spike means: 0.4936462342739105\n",
      "layer 2 spike means: 0.05953124910593033\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.0770\n",
      "layer 1 spike means: 0.48560792207717896\n",
      "layer 2 spike means: 0.07585937529802322\n",
      "Epoch 1 | Batch 900/938 | Loss: 2.0448\n",
      "layer 1 spike means: 0.49572449922561646\n",
      "layer 2 spike means: 0.08546874672174454\n",
      "Epoch 1 finished. Test accuracy: 67.97%\n",
      "Epoch 2 | Batch 100/938 | Loss: 1.7647\n",
      "layer 1 spike means: 0.48330995440483093\n",
      "layer 2 spike means: 0.06523437052965164\n",
      "Epoch 2 | Batch 200/938 | Loss: 1.7521\n",
      "layer 1 spike means: 0.48005372285842896\n",
      "layer 2 spike means: 0.07468749582767487\n",
      "Epoch 2 | Batch 300/938 | Loss: 1.7455\n",
      "layer 1 spike means: 0.4816436767578125\n",
      "layer 2 spike means: 0.0846875011920929\n",
      "Epoch 2 | Batch 400/938 | Loss: 1.7352\n",
      "layer 1 spike means: 0.49798280000686646\n",
      "layer 2 spike means: 0.09148437529802322\n",
      "Epoch 2 | Batch 500/938 | Loss: 1.7287\n",
      "layer 1 spike means: 0.5065246820449829\n",
      "layer 2 spike means: 0.0832812488079071\n",
      "Epoch 2 | Batch 600/938 | Loss: 1.7235\n",
      "layer 1 spike means: 0.494406133890152\n",
      "layer 2 spike means: 0.08601562678813934\n",
      "Epoch 2 | Batch 700/938 | Loss: 1.7193\n",
      "layer 1 spike means: 0.4864746034145355\n",
      "layer 2 spike means: 0.07679687440395355\n",
      "Epoch 2 | Batch 800/938 | Loss: 1.7151\n",
      "layer 1 spike means: 0.491659551858902\n",
      "layer 2 spike means: 0.08234374970197678\n",
      "Epoch 2 | Batch 900/938 | Loss: 1.7117\n",
      "layer 1 spike means: 0.4767822325229645\n",
      "layer 2 spike means: 0.08265624940395355\n",
      "Epoch 2 finished. Test accuracy: 78.94%\n",
      "Epoch 3 | Batch 100/938 | Loss: 1.6714\n",
      "layer 1 spike means: 0.48409730195999146\n",
      "layer 2 spike means: 0.08124999701976776\n",
      "Epoch 3 | Batch 200/938 | Loss: 1.6732\n",
      "layer 1 spike means: 0.49421998858451843\n",
      "layer 2 spike means: 0.07656250149011612\n",
      "Epoch 3 | Batch 300/938 | Loss: 1.6744\n",
      "layer 1 spike means: 0.5036072134971619\n",
      "layer 2 spike means: 0.08726562559604645\n",
      "Epoch 3 | Batch 400/938 | Loss: 1.6724\n",
      "layer 1 spike means: 0.4897522032260895\n",
      "layer 2 spike means: 0.08437500149011612\n",
      "Epoch 3 | Batch 500/938 | Loss: 1.6723\n",
      "layer 1 spike means: 0.4880432188510895\n",
      "layer 2 spike means: 0.0657031238079071\n",
      "Epoch 3 | Batch 600/938 | Loss: 1.6740\n",
      "layer 1 spike means: 0.481292724609375\n",
      "layer 2 spike means: 0.08945312350988388\n",
      "Epoch 3 | Batch 700/938 | Loss: 1.6730\n",
      "layer 1 spike means: 0.474295049905777\n",
      "layer 2 spike means: 0.09046874940395355\n",
      "Epoch 3 | Batch 800/938 | Loss: 1.6723\n",
      "layer 1 spike means: 0.46481019258499146\n",
      "layer 2 spike means: 0.07968749850988388\n",
      "Epoch 3 | Batch 900/938 | Loss: 1.6680\n",
      "layer 1 spike means: 0.466909795999527\n",
      "layer 2 spike means: 0.08281249552965164\n",
      "Epoch 3 finished. Test accuracy: 82.02%\n",
      "For hidden size 256 accuracy is -->  82.02\n",
      "Layer 0 W mean: -0.000647\n",
      "Layer 0 W std: 0.052950\n",
      "Layer 0 grad mean: 0.000000\n",
      "Layer 0 grad max: 0.001173\n",
      "Layer 1 W mean: -0.009427\n",
      "Layer 1 W std: 0.071238\n",
      "Layer 1 grad mean: -0.000207\n",
      "Layer 1 grad max: 0.007230\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256, 512]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [64, 256]\n",
    "threshold = 1\n",
    "epoch = 3\n",
    "T = 25\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [0.3, 0.3])\n",
    "    model, acc = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, super_spike_beta=1)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    for i, lif in enumerate(model.lif_layers):\n",
    "        print(f\"Layer {i} W mean: {lif.W.mean().item():.6f}\")\n",
    "        print(f\"Layer {i} W std: {lif.W.std().item():.6f}\")\n",
    "        if lif.W.grad is not None:\n",
    "            print(f\"Layer {i} grad mean: {lif.W.grad.mean().item():.6f}\")\n",
    "            print(f\"Layer {i} grad max: {lif.W.grad.abs().max().item():.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "be8719b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 | Batch 100/938 | Loss: 2.3101\n",
      "layer 1 spike means: 0.4888061583042145\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 200/938 | Loss: 2.3061\n",
      "layer 1 spike means: 0.4969848692417145\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 300/938 | Loss: 2.2947\n",
      "layer 1 spike means: 0.49171143770217896\n",
      "layer 2 spike means: 0.014609375037252903\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.2765\n",
      "layer 1 spike means: 0.5290283560752869\n",
      "layer 2 spike means: 0.010937499813735485\n",
      "Epoch 1 | Batch 500/938 | Loss: 2.2647\n",
      "layer 1 spike means: 0.537109375\n",
      "layer 2 spike means: 0.01249999925494194\n",
      "Epoch 1 | Batch 600/938 | Loss: 2.2571\n",
      "layer 1 spike means: 0.560839831829071\n",
      "layer 2 spike means: 0.008281249552965164\n",
      "Epoch 1 | Batch 700/938 | Loss: 2.2514\n",
      "layer 1 spike means: 0.566418468952179\n",
      "layer 2 spike means: 0.009609375149011612\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.2466\n",
      "layer 1 spike means: 0.5447143912315369\n",
      "layer 2 spike means: 0.010312499478459358\n",
      "Epoch 1 | Batch 900/938 | Loss: 2.2438\n",
      "layer 1 spike means: 0.532299816608429\n",
      "layer 2 spike means: 0.008593750186264515\n",
      "Epoch 1 finished. Test accuracy: 20.66%\n",
      "Epoch 2 | Batch 100/938 | Loss: 2.2137\n",
      "layer 1 spike means: 0.5119262933731079\n",
      "layer 2 spike means: 0.008671875111758709\n",
      "Epoch 2 | Batch 200/938 | Loss: 2.2178\n",
      "layer 1 spike means: 0.501757800579071\n",
      "layer 2 spike means: 0.007499999832361937\n",
      "Epoch 2 | Batch 300/938 | Loss: 2.2148\n",
      "layer 1 spike means: 0.4948974549770355\n",
      "layer 2 spike means: 0.010859374888241291\n",
      "Epoch 2 | Batch 400/938 | Loss: 2.2161\n",
      "layer 1 spike means: 0.501635730266571\n",
      "layer 2 spike means: 0.006093749776482582\n",
      "Epoch 2 | Batch 500/938 | Loss: 2.2179\n",
      "layer 1 spike means: 0.51129150390625\n",
      "layer 2 spike means: 0.003124999813735485\n",
      "Epoch 2 | Batch 600/938 | Loss: 2.2150\n",
      "layer 1 spike means: 0.514697253704071\n",
      "layer 2 spike means: 0.027812499552965164\n",
      "Epoch 2 | Batch 700/938 | Loss: 2.2099\n",
      "layer 1 spike means: 0.5473877191543579\n",
      "layer 2 spike means: 0.011874999850988388\n",
      "Epoch 2 | Batch 800/938 | Loss: 2.2002\n",
      "layer 1 spike means: 0.5323120355606079\n",
      "layer 2 spike means: 0.030312499031424522\n",
      "Epoch 2 | Batch 900/938 | Loss: 2.1904\n",
      "layer 1 spike means: 0.5472168326377869\n",
      "layer 2 spike means: 0.02046874910593033\n",
      "Epoch 2 finished. Test accuracy: 32.14%\n",
      "Epoch 3 | Batch 100/938 | Loss: 2.1001\n",
      "layer 1 spike means: 0.55224609375\n",
      "layer 2 spike means: 0.029843749478459358\n",
      "Epoch 3 | Batch 200/938 | Loss: 2.1005\n",
      "layer 1 spike means: 0.558703601360321\n",
      "layer 2 spike means: 0.030156249180436134\n",
      "Epoch 3 | Batch 300/938 | Loss: 2.1112\n",
      "layer 1 spike means: 0.5434326529502869\n",
      "layer 2 spike means: 0.015625\n",
      "Epoch 3 | Batch 400/938 | Loss: 2.1182\n",
      "layer 1 spike means: 0.540417492389679\n",
      "layer 2 spike means: 0.021640624850988388\n",
      "Epoch 3 | Batch 500/938 | Loss: 2.1214\n",
      "layer 1 spike means: 0.521374523639679\n",
      "layer 2 spike means: 0.02242187410593033\n",
      "Epoch 3 | Batch 600/938 | Loss: 2.1259\n",
      "layer 1 spike means: 0.525561511516571\n",
      "layer 2 spike means: 0.008671875111758709\n",
      "Epoch 3 | Batch 700/938 | Loss: 2.1291\n",
      "layer 1 spike means: 0.528796374797821\n",
      "layer 2 spike means: 0.01601562462747097\n",
      "Epoch 3 | Batch 800/938 | Loss: 2.1284\n",
      "layer 1 spike means: 0.547607421875\n",
      "layer 2 spike means: 0.019765624776482582\n",
      "Epoch 3 | Batch 900/938 | Loss: 2.1261\n",
      "layer 1 spike means: 0.56866455078125\n",
      "layer 2 spike means: 0.027968749403953552\n",
      "Epoch 3 finished. Test accuracy: 36.00%\n",
      "For hidden size 64 accuracy is -->  36.0\n",
      "Layer 0 W mean: 0.003536\n",
      "Layer 0 W std: 0.117907\n",
      "Layer 0 grad mean: 0.000000\n",
      "Layer 0 grad max: 0.000002\n",
      "Layer 1 W mean: -0.031369\n",
      "Layer 1 W std: 0.351346\n",
      "Layer 1 grad mean: -0.000517\n",
      "Layer 1 grad max: 0.011146\n",
      "Using device: cuda\n",
      "Epoch 1 | Batch 100/938 | Loss: 2.3020\n",
      "layer 1 spike means: 0.5233063101768494\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 200/938 | Loss: 2.3023\n",
      "layer 1 spike means: 0.5208191275596619\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 300/938 | Loss: 2.2979\n",
      "layer 1 spike means: 0.5202301144599915\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.2958\n",
      "layer 1 spike means: 0.512286365032196\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 500/938 | Loss: 2.2971\n",
      "layer 1 spike means: 0.5098022818565369\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 600/938 | Loss: 2.2980\n",
      "layer 1 spike means: 0.5106964111328125\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 700/938 | Loss: 2.2987\n",
      "layer 1 spike means: 0.5126067996025085\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.2993\n",
      "layer 1 spike means: 0.512432873249054\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 | Batch 900/938 | Loss: 2.2996\n",
      "layer 1 spike means: 0.516192615032196\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 1 finished. Test accuracy: 9.80%\n",
      "Epoch 2 | Batch 100/938 | Loss: 2.3026\n",
      "layer 1 spike means: 0.517822265625\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 2 | Batch 200/938 | Loss: 2.2990\n",
      "layer 1 spike means: 0.520581066608429\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 2 | Batch 300/938 | Loss: 2.3002\n",
      "layer 1 spike means: 0.521453857421875\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 2 | Batch 400/938 | Loss: 2.2998\n",
      "layer 1 spike means: 0.5174224972724915\n",
      "layer 2 spike means: 0.0078125\n",
      "Epoch 2 | Batch 500/938 | Loss: 2.2853\n",
      "layer 1 spike means: 0.5202606320381165\n",
      "layer 2 spike means: 0.0015624999068677425\n",
      "Epoch 2 | Batch 600/938 | Loss: 2.2740\n",
      "layer 1 spike means: 0.5185211300849915\n",
      "layer 2 spike means: 0.010703125037252903\n",
      "Epoch 2 | Batch 700/938 | Loss: 2.2667\n",
      "layer 1 spike means: 0.5217987298965454\n",
      "layer 2 spike means: 0.003124999813735485\n",
      "Epoch 2 | Batch 800/938 | Loss: 2.2710\n",
      "layer 1 spike means: 0.5224456787109375\n",
      "layer 2 spike means: 0.0\n",
      "Epoch 2 | Batch 900/938 | Loss: 2.2744\n",
      "layer 1 spike means: 0.5199981927871704\n",
      "layer 2 spike means: 0.004687499720603228\n",
      "Epoch 2 finished. Test accuracy: 9.80%\n",
      "Epoch 3 | Batch 100/938 | Loss: 2.2833\n",
      "layer 1 spike means: 0.52459716796875\n",
      "layer 2 spike means: 0.01249999925494194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hidden_size \u001b[38;5;129;01min\u001b[39;00m hidden_sizes:\n\u001b[32m     11\u001b[39m     model, optimizer = get_model([\u001b[32m784\u001b[39m, hidden_size, \u001b[32m10\u001b[39m], thresholds = [\u001b[32m0.3\u001b[39m, \u001b[32m0.3\u001b[39m], lr = \u001b[32m0.005\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     model, acc = \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_spike_beta\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFor hidden size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m accuracy is --> \u001b[39m\u001b[33m\"\u001b[39m, acc)\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, lif \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(model.lif_layers):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(train_loader, test_loader, model, optimizer, num_epochs, verbose, super_spike_beta)\u001b[39m\n\u001b[32m     14\u001b[39m data = data.view(data.size(\u001b[32m0\u001b[39m), -\u001b[32m1\u001b[39m)\n\u001b[32m     15\u001b[39m data = data.unsqueeze(\u001b[32m0\u001b[39m).repeat(\u001b[32m20\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (T, B, in_dim), rate encoding\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m total_loss += \u001b[43mrandom_bp_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_learning_window\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuper_spike_B\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuper_spike_beta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx + \u001b[32m1\u001b[39m) % \u001b[32m100\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (verbose):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[72]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mrandom_bp_step\u001b[39m\u001b[34m(model, x, target, optimizer, super_spike_B, last_learning_window)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mmodel -> all_layers(RandomBPSNN)\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mx -> (B, in_dim) or (T, B, in_dim) if it is (B, in_dim) timesteps are produced using rate-encoding\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m \u001b[33;03mdecreasing it -> network only pays attention to last spikes, the latency is longer because it waits for the last spikes\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m    \n\u001b[32m     13\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m out_spikes = out[\u001b[33m\"\u001b[39m\u001b[33mo_spk\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;66;03m# (T, B, 10)\u001b[39;00m\n\u001b[32m     17\u001b[39m T, B, num_classes = out_spikes.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alper\\Desktop\\AIN313\\3\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alper\\Desktop\\AIN313\\3\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mRandomBPSNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m spikes, voltages = [], []\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m lif \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.lif_layers:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     o_spk, o_U = \u001b[43mlif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo_spk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m     spikes.append(o_spk)\n\u001b[32m     39\u001b[39m     voltages.append(o_U)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alper\\Desktop\\AIN313\\3\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alper\\Desktop\\AIN313\\3\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mLIFLayer.forward\u001b[39m\u001b[34m(self, input_spikes)\u001b[39m\n\u001b[32m     39\u001b[39m I = \u001b[38;5;28mself\u001b[39m.alpha * I + x_t @ \u001b[38;5;28mself\u001b[39m.W.t()  \u001b[38;5;66;03m# (B, out_features)\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m#new membrane voltage\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m U = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbeta\u001b[49m * U + I\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m#if U exceeds threshold get one as a spike.\u001b[39;00m\n\u001b[32m     45\u001b[39m S = spike_fn(U, \u001b[38;5;28mself\u001b[39m.threshold)  \u001b[38;5;66;03m# (B, out_features)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Alper\\Desktop\\AIN313\\3\\pytorch_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1918\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1909\u001b[39m         \u001b[38;5;28mself\u001b[39m._backward_pre_hooks = OrderedDict()\n\u001b[32m   1911\u001b[39m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[32m   1912\u001b[39m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[32m   1913\u001b[39m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1916\u001b[39m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[32m   1917\u001b[39m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1918\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Any:\n\u001b[32m   1919\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m:\n\u001b[32m   1920\u001b[39m         _parameters = \u001b[38;5;28mself\u001b[39m.\u001b[34m__dict__\u001b[39m[\u001b[33m\"\u001b[39m\u001b[33m_parameters\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256, 512]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [64, 256]\n",
    "threshold = 1\n",
    "epoch = 3\n",
    "T = 25\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [0.3, 0.3], lr = 0.005)\n",
    "    model, acc = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, super_spike_beta=1)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    for i, lif in enumerate(model.lif_layers):\n",
    "        print(f\"Layer {i} W mean: {lif.W.mean().item():.6f}\")\n",
    "        print(f\"Layer {i} W std: {lif.W.std().item():.6f}\")\n",
    "        if lif.W.grad is not None:\n",
    "            print(f\"Layer {i} grad mean: {lif.W.grad.mean().item():.6f}\")\n",
    "            print(f\"Layer {i} grad max: {lif.W.grad.abs().max().item():.6f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "966d6019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  88.33\n",
      "Layer 0 density ->  tensor(0.4982)\n",
      "Layer 1 density ->  tensor(0.0909)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  90.09\n",
      "Layer 0 density ->  tensor(0.5068)\n",
      "Layer 1 density ->  tensor(0.0897)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  87.1\n",
      "Layer 0 density ->  tensor(0.5224)\n",
      "Layer 1 density ->  tensor(0.0915)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  88.13\n",
      "Layer 0 density ->  tensor(0.4924)\n",
      "Layer 1 density ->  tensor(0.0936)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [0.3, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.1)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d4e8e1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  78.48\n",
      "Layer 0 density ->  tensor(0.5881)\n",
      "Layer 1 density ->  tensor(0.0547)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  73.9\n",
      "Layer 0 density ->  tensor(0.5333)\n",
      "Layer 1 density ->  tensor(0.0719)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  57.51\n",
      "Layer 0 density ->  tensor(0.4903)\n",
      "Layer 1 density ->  tensor(0.0437)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  35.22\n",
      "Layer 0 density ->  tensor(0.5136)\n",
      "Layer 1 density ->  tensor(0.0217)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [0.1, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.5)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2d40ebae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  88.15\n",
      "Layer 0 density ->  tensor(0.5238)\n",
      "Layer 1 density ->  tensor(0.0866)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  90.25\n",
      "Layer 0 density ->  tensor(0.4979)\n",
      "Layer 1 density ->  tensor(0.0899)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  91.07\n",
      "Layer 0 density ->  tensor(0.4666)\n",
      "Layer 1 density ->  tensor(0.0907)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  90.19\n",
      "Layer 0 density ->  tensor(0.4974)\n",
      "Layer 1 density ->  tensor(0.0921)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [0.05, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.1)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a13d59e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  80.14\n",
      "Layer 0 density ->  tensor(0.4716)\n",
      "Layer 1 density ->  tensor(0.0677)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  65.38\n",
      "Layer 0 density ->  tensor(0.5717)\n",
      "Layer 1 density ->  tensor(0.0484)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  39.09\n",
      "Layer 0 density ->  tensor(0.4675)\n",
      "Layer 1 density ->  tensor(0.0220)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  47.2\n",
      "Layer 0 density ->  tensor(0.4533)\n",
      "Layer 1 density ->  tensor(0.0394)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [1, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.7)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c449a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  55.56\n",
      "Layer 0 density ->  tensor(0.5825)\n",
      "Layer 1 density ->  tensor(0.0433)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  46.33\n",
      "Layer 0 density ->  tensor(0.4838)\n",
      "Layer 1 density ->  tensor(0.0428)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  18.49\n",
      "Layer 0 density ->  tensor(0.4097)\n",
      "Layer 1 density ->  tensor(0.0089)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  12.57\n",
      "Layer 0 density ->  tensor(0.4510)\n",
      "Layer 1 density ->  tensor(0.0079)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [2, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=1.2)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3b023c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  91.82\n",
      "Layer 0 density ->  tensor(0.5716)\n",
      "Layer 1 density ->  tensor(0.0924)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  92.17\n",
      "Layer 0 density ->  tensor(0.5054)\n",
      "Layer 1 density ->  tensor(0.0894)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  92.96\n",
      "Layer 0 density ->  tensor(0.4625)\n",
      "Layer 1 density ->  tensor(0.0936)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  93.78\n",
      "Layer 0 density ->  tensor(0.4687)\n",
      "Layer 1 density ->  tensor(0.0959)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [2, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.02)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cd59eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  91.47\n",
      "Layer 0 density ->  tensor(0.4994)\n",
      "Layer 1 density ->  tensor(0.0860)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  90.5\n",
      "Layer 0 density ->  tensor(0.5081)\n",
      "Layer 1 density ->  tensor(0.0858)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  91.76\n",
      "Layer 0 density ->  tensor(0.3898)\n",
      "Layer 1 density ->  tensor(0.0862)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  92.93\n",
      "Layer 0 density ->  tensor(0.3861)\n",
      "Layer 1 density ->  tensor(0.0927)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model([784, hidden_size, 10], thresholds = [5, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = main(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, super_spike_beta=0.02)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "263e5436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.autograd import Function\n",
    "from torch.nn import functional as F\n",
    "import math\n",
    "\n",
    "def spike_fn(U, threshold):\n",
    "    return (U >= threshold).float()\n",
    "\n",
    "\n",
    "#input X spikes is determined by the probability,\n",
    "#higher the pixel tensity, higher the probability there is a spike.\n",
    "def poisson_encode(x, T):\n",
    "    x = x.unsqueeze(0).repeat(T, 1, 1)  # (T, B, features)\n",
    "    spikes = (torch.rand_like(x) < x).float()\n",
    "    return spikes\n",
    "\n",
    "\n",
    "#Now, I implement it with eligibility, because with the last learning window approach, the networks was enforced to\n",
    "#produce spikes at the ends, however there was no temporal information.\n",
    "#So, we either use Backpropagation through time, or eligibility trace.\n",
    "#backpropagation through time is not biologically plausible, so we use eligibility trace.\n",
    "#Also, a good thing is that eligibility trace and backpropagation through time is\n",
    "#roughly equivalent to each other mathematically(roughly, not exactly, works in practice).\n",
    "#eligibility trace basically -> you put a trace to the neurons at each time stop, basicaly it makes the neuron know the\n",
    "#gradients of the previous time step. So it is why it is equivalent to backpropagation through time.\n",
    "#each time step we multiply that trace from the previous time step with a number, so it may cause vanishing gradients if\n",
    "#that factor(gamma) is < 1 , or exploding gradients if gamma > 1, \n",
    "#it is used for holding the temporal context.\n",
    "#also one last thing to note it makes it online, so no need to hold the history, less memory need and biologicaly more plausible\n",
    "class LIFLayerEligibility(nn.Module):\n",
    "    def __init__(self, in_features, out_features, tau_trace = 20.0,\n",
    "                 tau_mem=20.0, tau_syn=5.0, dt=1.0, threshold=1.0, super_spike_B = 0.03):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.threshold = threshold\n",
    "        self.super_spike_B = super_spike_B\n",
    "        # use Xavier initialization, because otherwise some neurons were not firing at all, it was unstable.\n",
    "        std = 1 / (in_features**0.5)\n",
    "        self.W = nn.Parameter(torch.randn(out_features, in_features) * std)\n",
    "        \n",
    "\n",
    "        self.dt = dt #just a theoretical constant\n",
    "\n",
    "        #Use register_buffer, because it must be linked to the nn.Module\n",
    "        self.register_buffer('alpha', torch.exp(torch.tensor(-dt / tau_syn))) #Voltage decay\n",
    "        self.register_buffer('beta', torch.exp(torch.tensor(-dt / tau_mem))) #Current decay\n",
    "        self.register_buffer('gamma', torch.exp(torch.tensor(-dt / tau_trace)))  #Trace decay\n",
    "\n",
    "\n",
    "    #input spikes -> (T, B, in_dim)\n",
    "    #returns -> (T, B, out_dim) as spikes, and (T, B, out_dim) as voltage values\n",
    "    def forward(self, input_spikes):\n",
    "\n",
    "        T, B, _ = input_spikes.shape\n",
    "        device = self.W.device\n",
    "\n",
    "        #current, and voltage must start from 0 in the beginning\n",
    "        I = torch.zeros(B, self.out_features, device=device)\n",
    "        U = torch.zeros(B, self.out_features, device=device)\n",
    "\n",
    "        #keep all spike and voltage values across the time steps.\n",
    "        out_spikes = []\n",
    "        U_hist = []\n",
    "        trace = torch.zeros(B, self.out_features, self.in_features, device=device)\n",
    "        for t in range(T):\n",
    "            x_t = input_spikes[t]  # (B, in_features), X value of the current time step.\n",
    "\n",
    "            #new current\n",
    "            I = self.alpha * I + x_t @ self.W.t()  # (B, out_features)\n",
    "\n",
    "            #new membrane voltage\n",
    "            U = self.beta * U + I\n",
    "\n",
    "            #if U exceeds threshold get one as a spike.\n",
    "            S = spike_fn(U, self.threshold)  # (B, out_features)\n",
    "            U_hist.append(U.clone())\n",
    "            U = U - S * self.threshold #reset U\n",
    "            # Surrogate gradient\n",
    "            du = (U - self.threshold).abs()\n",
    "            sigma_prime = 1 / ((1 + self.super_spike_B * du)**2)  # (B, out_features)\n",
    "\n",
    "            # Eligibility trace update\n",
    "            # calculate trace here\n",
    "            trace = self.gamma * trace + sigma_prime.unsqueeze(2) * x_t.unsqueeze(1)\n",
    "            \n",
    "            U_hist.append(U.clone())\n",
    "            U = U - S * self.threshold\n",
    "            out_spikes.append(S)\n",
    "\n",
    "            \n",
    "\n",
    "        out_spikes = torch.stack(out_spikes, dim=0)\n",
    "        U_hist = torch.stack(U_hist, dim=0)\n",
    "        return out_spikes, U_hist, trace\n",
    "\n",
    "class RandomBPSNNEligibility(nn.Module):\n",
    "    def __init__(self, T=20):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "        self.lif_layers = torch.nn.ModuleList()\n",
    "        self.G_hiddens = torch.nn.ParameterList()\n",
    "\n",
    "\n",
    "    def append_LIF(self, LIF, out_dim = 10):\n",
    "        self.lif_layers.append(LIF)\n",
    "        if (len(self.lif_layers) > 1):\n",
    "            hidden_dim = self.lif_layers[-2].out_features #set the projection matrix\n",
    "            #random projection matrix -> (out_dim(10), hidden_dim)\n",
    "            G_hidden = nn.Parameter(\n",
    "                torch.randn(out_dim, hidden_dim) * 0.1, requires_grad = False\n",
    "            )\n",
    "            self.G_hiddens.append(G_hidden)\n",
    "\n",
    "    #here if X is a static no temporal data -> X values are encoded using rate-encoding or poisson encoding\n",
    "    #generally rate-encoding is used because it is more stable however I implemented poisson encoding above as well.\n",
    "    #if it has a stime steps, no encoding is used.\n",
    "\n",
    "    #Static X -> (B, in_dim)\n",
    "    #Time_step_X -> (T, B, in_dim)\n",
    "    def forward(self, x):\n",
    "        device = self.lif_layers[0].W.device\n",
    "        static = len(x.shape) == 2\n",
    "        x = x.to(device)\n",
    "        if (static):\n",
    "            B, in_dim = x.shape\n",
    "            x = x.unsqueeze(0).repeat(self.T, 1, 1)  # (T, B, in_dim), rate encoding\n",
    "\n",
    "        else:\n",
    "            _, B, in_dim = x.shape\n",
    "        o_spk = x\n",
    "        spikes, voltages, traces = [], [], []\n",
    "        for lif in self.lif_layers:\n",
    "            o_spk, o_U, trace = lif(o_spk)\n",
    "            spikes.append(o_spk)\n",
    "            voltages.append(o_U)\n",
    "            traces.append(trace)\n",
    "\n",
    "        #to calculate accuracy, we generally sum spike numbers of all the time steps, and predict the \n",
    "        #class which has maximum number of summed spikies.\n",
    "        out_rate = o_spk.mean(dim=0)  #(B, O)\n",
    "\n",
    "        return {\n",
    "            \"Us\": voltages,\n",
    "            \"spikes\": spikes,\n",
    "            \"o_spk\": spikes[-1],\n",
    "            \"o_U\": voltages[-1],\n",
    "            \"out_rate\": out_rate,\n",
    "            \"traces\": traces\n",
    "        }\n",
    "the_device = torch.device(\"cuda\")\n",
    "def random_bp_step_eligibility(model, x, target, optimizer):\n",
    "    \"\"\"\n",
    "    model -> all_layers(RandomBPSNN)\n",
    "    x -> (B, in_dim) or (T, B, in_dim) if it is (B, in_dim) timesteps are produced using rate-encoding\n",
    "    target -> (B, out_dim)\n",
    "    loss_fn -> Cross entropy derivative is used here -> |Y - y_predicted|\n",
    "    \"\"\"    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    x = x.to(the_device)\n",
    "    out = model(x)\n",
    "    spikes = out[\"spikes\"] # (layer_num, T, B, O)\n",
    "    traces = out[\"traces\"] # (layer_num, ) -> trace values of the last time step for each layers, \n",
    "    out_spikes = out[\"o_spk\"] # (T, B, 10)\n",
    "    T, B, num_classes = out_spikes.shape\n",
    "    \n",
    "    #apply one_hot encoding if it hasnt applied yet.\n",
    "    if (len(target.shape) == 1):\n",
    "        target = F.one_hot(target, num_classes=num_classes).float()  # (B, 10)\n",
    "\n",
    "    #calculate errors\n",
    "    out_rate = spikes[-1].mean(dim=0)  # (B, num_classes)\n",
    "    error = out_rate - target           # (B, num_classes)\n",
    "    loss = F.cross_entropy(out_rate, target)\n",
    "\n",
    "    #calculate gradients from the traces\n",
    "    for i, lif in enumerate(model.lif_layers):\n",
    "        trace = traces[i]  #(B, out, in)\n",
    "        \n",
    "        #if it is output layer just use normal error value, if it is a hidden layer use projection of the error derivative\n",
    "        if i == len(model.lif_layers) - 1:\n",
    "            #Output layer -> dW = error * trace\n",
    "            dW = torch.einsum('bo,boi->oi', error, trace) / B\n",
    "        else:\n",
    "            #Hidden layer: project error with random matrix\n",
    "            G = model.G_hiddens[i]\n",
    "            proj_error = error @ G  # (B, hidden)\n",
    "            dW = torch.einsum('bh,bhi->hi', proj_error, trace) / B\n",
    "        \n",
    "        lif.W.grad = dW\n",
    "    \n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "#train the model, with eligibility applied\n",
    "def train_eligibility(train_loader, test_loader, model, optimizer, num_epochs, verbose = True, apply_poisson = False, T = 20, dont_touch = False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    spike_means_outer = []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data = data.to(device)   # (B,1,28,28)\n",
    "            target = target.to(device)\n",
    "            if not dont_touch:\n",
    "                # Flatten: (B, 784)\n",
    "                data = data.view(data.size(0), -1)\n",
    "\n",
    "                if (not apply_poisson):\n",
    "                # Flatten: (B, 784)\n",
    "                    data = data.unsqueeze(0).repeat(T, 1, 1)  # (T, B, in_dim), rate encoding\n",
    "                else:\n",
    "                    data = poisson_encode(data, T = T)\n",
    "                    \n",
    "\n",
    "            total_loss += random_bp_step_eligibility(model, data, target, optimizer)\n",
    "\n",
    "            if (batch_idx + 1) % 8 == 0:\n",
    "                if (verbose):\n",
    "                    print(f\"Epoch {epoch} | Batch {batch_idx+1}/{len(train_loader)} | \"\n",
    "                        f\"Loss: {total_loss / (batch_idx+1):.4f}\")\n",
    "                data, target = next(iter(train_loader))\n",
    "                if (not dont_touch):\n",
    "                    data = data.to(device).view(data.size(0), -1)\n",
    "\n",
    "                out = model(data)\n",
    "                spike_means = []\n",
    "                for i in out[\"spikes\"]:\n",
    "                    spike_means.append(i.mean().item())\n",
    "                spike_means_outer.append(torch.tensor(spike_means))\n",
    "                if (verbose):\n",
    "                    for i in range(len(spike_means)):\n",
    "                        print(f\"layer {i+1} spike means:\", spike_means[i])\n",
    "\n",
    "\n",
    "        #calculate accuracy\n",
    "        acc = evaluate(model, test_loader, device)\n",
    "        if (verbose):\n",
    "            print(f\"Epoch {epoch} finished. Test accuracy: {acc:.2f}%\")\n",
    "    spike_means_outer = torch.stack(spike_means_outer, dim = 0)\n",
    "    return model, acc, spike_means_outer\n",
    "def evaluate(model, loader, device, dont_touch = False):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data = data.to(device)\n",
    "            target = target.to(device)\n",
    "            if (not dont_touch):\n",
    "                data = data.view(data.size(0), -1) #it is not already flattened, so flatten it.\n",
    "\n",
    "            out = model(data)\n",
    "            out_rate = out[\"out_rate\"]  # (B, out_dim)\n",
    "            pred = out_rate.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "#returns a model and optimizer,\n",
    "#you can determine the hidden layer number with hidden_dims\n",
    "#hidden dims -> [X, Y, Z, T, U] it means that input layer is X; Y, Z, T are neuron numbers of the hidden layers, and U is out_dim\n",
    "#you can set different thresholds for each of the hidden_layer, default is 1 threshold\n",
    "def get_model_eligibility(hidden_dims, thresholds = None, super_spike_beta = 25, lr = 1e-3, out_dims = 10):\n",
    "    if (thresholds is None):\n",
    "        thresholds = [1 for _ in range(len(hidden_dims) - 1)]\n",
    "\n",
    "    model = RandomBPSNNEligibility()\n",
    "    weights = []\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "        model.append_LIF(LIFLayerEligibility(hidden_dims[i-1], hidden_dims[i], threshold=thresholds[i-1], super_spike_B=super_spike_beta), out_dim=out_dims)\n",
    "        weights.append(model.lif_layers[-1].W)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        weights, lr=lr\n",
    "    )\n",
    "    return model, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1130f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  89.99\n",
      "Layer 0 density ->  tensor(0.4973)\n",
      "Layer 1 density ->  tensor(0.0713)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  90.94\n",
      "Layer 0 density ->  tensor(0.4743)\n",
      "Layer 1 density ->  tensor(0.0826)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  89.81\n",
      "Layer 0 density ->  tensor(0.4545)\n",
      "Layer 1 density ->  tensor(0.0892)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  91.5\n",
      "Layer 0 density ->  tensor(0.4784)\n",
      "Layer 1 density ->  tensor(0.0861)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model_eligibility([784, hidden_size, 10], thresholds = [5, 0.3], lr = 0.005, out_dims=10)\n",
    "    model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, dont_touch=False)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61095d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "accuracy is -->  89.19\n",
      "Layer 0 density ->  tensor(0.5298)\n",
      "Layer 1 density ->  tensor(0.5233)\n",
      "Layer 2 density ->  tensor(0.0674)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 2\n",
    "model, optimizer = get_model_eligibility([784, 256, 64, 10], thresholds = [5, 4, 0.3], lr = 0.005)\n",
    "model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False, apply_poisson=True)\n",
    "print(f\"accuracy is --> \", acc)\n",
    "spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "for i in range(len(spikes_means)):\n",
    "    print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a7e47d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 | Batch 400/938 | Loss: 1.8441\n",
      "layer 1 spike means: 0.44755861163139343\n",
      "layer 2 spike means: 0.1049218699336052\n",
      "Epoch 1 | Batch 800/938 | Loss: 1.7337\n",
      "layer 1 spike means: 0.46051025390625\n",
      "layer 2 spike means: 0.09765625\n",
      "Epoch 1 finished. Test accuracy: 88.97%\n",
      "Epoch 2 | Batch 400/938 | Loss: 1.5974\n",
      "layer 1 spike means: 0.48680421710014343\n",
      "layer 2 spike means: 0.09023437649011612\n",
      "Epoch 2 | Batch 800/938 | Loss: 1.5966\n",
      "layer 1 spike means: 0.5238281488418579\n",
      "layer 2 spike means: 0.09242187440395355\n",
      "Epoch 2 finished. Test accuracy: 90.02%\n",
      "accuracy is -->  90.02\n",
      "Layer 0 density ->  tensor(0.4797)\n",
      "Layer 1 density ->  tensor(0.0963)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 2\n",
    "model, optimizer = get_model_eligibility([784, 64, 10], thresholds = [3, 0.09], lr = 0.005)\n",
    "\n",
    "model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, apply_poisson=False)\n",
    "\n",
    "print(f\"accuracy is --> \", acc)\n",
    "spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "for i in range(len(spikes_means)):\n",
    "    print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "51b196ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "For hidden size 32 accuracy is -->  79.7\n",
      "Layer 0 density ->  tensor(0.5751)\n",
      "Layer 1 density ->  tensor(0.0783)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 64 accuracy is -->  88.27\n",
      "Layer 0 density ->  tensor(0.4855)\n",
      "Layer 1 density ->  tensor(0.0893)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 128 accuracy is -->  89.65\n",
      "Layer 0 density ->  tensor(0.4396)\n",
      "Layer 1 density ->  tensor(0.0884)\n",
      "\n",
      "Using device: cuda\n",
      "For hidden size 256 accuracy is -->  81.75\n",
      "Layer 0 density ->  tensor(0.4485)\n",
      "Layer 1 density ->  tensor(0.0795)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#experiment hidden sizes\n",
    "#hidden_sizes = [32, 64, 128, 256]\n",
    "# Model\n",
    "\n",
    "hidden_sizes = [32, 64, 128, 256]\n",
    "epoch = 2\n",
    "acc_results = []\n",
    "for hidden_size in hidden_sizes:\n",
    "    model, optimizer = get_model_eligibility([784, hidden_size, 10], thresholds = [50, 0.3], lr = 0.005)\n",
    "    model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=False)\n",
    "    print(f\"For hidden size {hidden_size} accuracy is --> \", acc)\n",
    "    spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "    for i in range(len(spikes_means)):\n",
    "        print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65892dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.3384\n",
      "layer 1 spike means: 0.5126953125\n",
      "layer 2 spike means: 0.4975341856479645\n",
      "layer 3 spike means: 0.014843749813735485\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.2484\n",
      "layer 1 spike means: 0.591748058795929\n",
      "layer 2 spike means: 0.5405029654502869\n",
      "layer 3 spike means: 0.055937498807907104\n",
      "Epoch 1 finished. Test accuracy: 54.11%\n",
      "Epoch 2 | Batch 400/938 | Loss: 1.9369\n",
      "layer 1 spike means: 0.5823730826377869\n",
      "layer 2 spike means: 0.5014892816543579\n",
      "layer 3 spike means: 0.10312499850988388\n",
      "Epoch 2 | Batch 800/938 | Loss: 1.8860\n",
      "layer 1 spike means: 0.617846667766571\n",
      "layer 2 spike means: 0.4841064512729645\n",
      "layer 3 spike means: 0.09921874850988388\n",
      "Epoch 2 finished. Test accuracy: 82.28%\n",
      "Epoch 3 | Batch 400/938 | Loss: 1.7000\n",
      "layer 1 spike means: 0.609375\n",
      "layer 2 spike means: 0.47395020723342896\n",
      "layer 3 spike means: 0.10468749701976776\n",
      "Epoch 3 | Batch 800/938 | Loss: 1.6821\n",
      "layer 1 spike means: 0.6128906607627869\n",
      "layer 2 spike means: 0.4695800840854645\n",
      "layer 3 spike means: 0.10187499970197678\n",
      "Epoch 3 finished. Test accuracy: 89.15%\n",
      "Epoch 4 | Batch 400/938 | Loss: 1.6296\n",
      "layer 1 spike means: 0.595104992389679\n",
      "layer 2 spike means: 0.4457031190395355\n",
      "layer 3 spike means: 0.09828124940395355\n",
      "Epoch 4 | Batch 800/938 | Loss: 1.6289\n",
      "layer 1 spike means: 0.5869385004043579\n",
      "layer 2 spike means: 0.47441408038139343\n",
      "layer 3 spike means: 0.10320312529802322\n",
      "Epoch 4 finished. Test accuracy: 90.22%\n",
      "Epoch 5 | Batch 400/938 | Loss: 1.6117\n",
      "layer 1 spike means: 0.5865112543106079\n",
      "layer 2 spike means: 0.4928222596645355\n",
      "layer 3 spike means: 0.10015624761581421\n",
      "Epoch 5 | Batch 800/938 | Loss: 1.6091\n",
      "layer 1 spike means: 0.5941162109375\n",
      "layer 2 spike means: 0.4783691465854645\n",
      "layer 3 spike means: 0.09914062172174454\n",
      "Epoch 5 finished. Test accuracy: 91.06%\n",
      "Epoch 6 | Batch 400/938 | Loss: 1.6005\n",
      "layer 1 spike means: 0.605090320110321\n",
      "layer 2 spike means: 0.5059570670127869\n",
      "layer 3 spike means: 0.09937499463558197\n",
      "Epoch 6 | Batch 800/938 | Loss: 1.5976\n",
      "layer 1 spike means: 0.59271240234375\n",
      "layer 2 spike means: 0.505175769329071\n",
      "layer 3 spike means: 0.09898437559604645\n",
      "Epoch 6 finished. Test accuracy: 91.97%\n",
      "Epoch 7 | Batch 400/938 | Loss: 1.5880\n",
      "layer 1 spike means: 0.5808472037315369\n",
      "layer 2 spike means: 0.4722900390625\n",
      "layer 3 spike means: 0.1008593738079071\n",
      "Epoch 7 | Batch 800/938 | Loss: 1.5863\n",
      "layer 1 spike means: 0.5875244140625\n",
      "layer 2 spike means: 0.48149415850639343\n",
      "layer 3 spike means: 0.10132811963558197\n",
      "Epoch 7 finished. Test accuracy: 92.43%\n",
      "Epoch 8 | Batch 400/938 | Loss: 1.5804\n",
      "layer 1 spike means: 0.5743164420127869\n",
      "layer 2 spike means: 0.5099853873252869\n",
      "layer 3 spike means: 0.0990624949336052\n",
      "Epoch 8 | Batch 800/938 | Loss: 1.5807\n",
      "layer 1 spike means: 0.5851196646690369\n",
      "layer 2 spike means: 0.501538097858429\n",
      "layer 3 spike means: 0.09953124821186066\n",
      "Epoch 8 finished. Test accuracy: 92.99%\n",
      "Epoch 9 | Batch 400/938 | Loss: 1.5723\n",
      "layer 1 spike means: 0.5800415277481079\n",
      "layer 2 spike means: 0.488037109375\n",
      "layer 3 spike means: 0.0971093699336052\n",
      "Epoch 9 | Batch 800/938 | Loss: 1.5733\n",
      "layer 1 spike means: 0.5844482779502869\n",
      "layer 2 spike means: 0.506518542766571\n",
      "layer 3 spike means: 0.10015624761581421\n",
      "Epoch 9 finished. Test accuracy: 93.05%\n",
      "Epoch 10 | Batch 400/938 | Loss: 1.5678\n",
      "layer 1 spike means: 0.5863891839981079\n",
      "layer 2 spike means: 0.5222412347793579\n",
      "layer 3 spike means: 0.10187499970197678\n",
      "Epoch 10 | Batch 800/938 | Loss: 1.5681\n",
      "layer 1 spike means: 0.577746570110321\n",
      "layer 2 spike means: 0.488037109375\n",
      "layer 3 spike means: 0.10039062052965164\n",
      "Epoch 10 finished. Test accuracy: 93.63%\n",
      "Epoch 11 | Batch 400/938 | Loss: 1.5635\n",
      "layer 1 spike means: 0.5939697623252869\n",
      "layer 2 spike means: 0.5150146484375\n",
      "layer 3 spike means: 0.09882812201976776\n",
      "Epoch 11 | Batch 800/938 | Loss: 1.5618\n",
      "layer 1 spike means: 0.586291491985321\n",
      "layer 2 spike means: 0.521655261516571\n",
      "layer 3 spike means: 0.09812499582767487\n",
      "Epoch 11 finished. Test accuracy: 93.69%\n",
      "Epoch 12 | Batch 400/938 | Loss: 1.5596\n",
      "layer 1 spike means: 0.589404284954071\n",
      "layer 2 spike means: 0.4987548887729645\n",
      "layer 3 spike means: 0.09718749672174454\n",
      "Epoch 12 | Batch 800/938 | Loss: 1.5586\n",
      "layer 1 spike means: 0.5990967154502869\n",
      "layer 2 spike means: 0.5043701529502869\n",
      "layer 3 spike means: 0.10249999910593033\n",
      "Epoch 12 finished. Test accuracy: 93.89%\n",
      "Epoch 13 | Batch 400/938 | Loss: 1.5539\n",
      "layer 1 spike means: 0.5948486328125\n",
      "layer 2 spike means: 0.531201183795929\n",
      "layer 3 spike means: 0.09937499463558197\n",
      "Epoch 13 | Batch 800/938 | Loss: 1.5529\n",
      "layer 1 spike means: 0.583752453327179\n",
      "layer 2 spike means: 0.5047363638877869\n",
      "layer 3 spike means: 0.1042187511920929\n",
      "Epoch 13 finished. Test accuracy: 94.11%\n",
      "Epoch 14 | Batch 400/938 | Loss: 1.5527\n",
      "layer 1 spike means: 0.565185546875\n",
      "layer 2 spike means: 0.522290050983429\n",
      "layer 3 spike means: 0.10117187350988388\n",
      "Epoch 14 | Batch 800/938 | Loss: 1.5525\n",
      "layer 1 spike means: 0.5912719964981079\n",
      "layer 2 spike means: 0.5123535394668579\n",
      "layer 3 spike means: 0.1003125011920929\n",
      "Epoch 14 finished. Test accuracy: 94.19%\n",
      "Epoch 15 | Batch 400/938 | Loss: 1.5496\n",
      "layer 1 spike means: 0.596850574016571\n",
      "layer 2 spike means: 0.511303722858429\n",
      "layer 3 spike means: 0.09703125059604645\n",
      "Epoch 15 | Batch 800/938 | Loss: 1.5493\n",
      "layer 1 spike means: 0.593640148639679\n",
      "layer 2 spike means: 0.5179687738418579\n",
      "layer 3 spike means: 0.09874999523162842\n",
      "Epoch 15 finished. Test accuracy: 94.33%\n",
      "Num layers: 3\n",
      "Num G matrices: 2\n",
      "Layer 0: 784 â 64\n",
      "Layer 1: 64 â 32\n",
      "Layer 2: 32 â 10\n",
      "G[0]: torch.Size([10, 64])\n",
      "G[1]: torch.Size([10, 32])\n",
      "Layer 0 grad mean: 0.000039\n",
      "Layer 0 grad max: 0.001950\n",
      "Layer 1 grad mean: 0.000364\n",
      "Layer 1 grad max: 0.002291\n",
      "Layer 2 grad mean: 0.000638\n",
      "Layer 2 grad max: 0.004784\n",
      "accuracy is -->  94.33\n",
      "Layer 0 density ->  tensor(0.5879)\n",
      "Layer 1 density ->  tensor(0.4993)\n",
      "Layer 2 density ->  tensor(0.0959)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 15\n",
    "model, optimizer = get_model_eligibility([784, 64, 32, 10], thresholds = [3, 2, 0.09], lr = 5e-4)\n",
    "\n",
    "model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, apply_poisson=False)\n",
    "\n",
    "print(f\"Num layers: {len(model.lif_layers)}\")\n",
    "print(f\"Num G matrices: {len(model.G_hiddens)}\")\n",
    "\n",
    "for i, lif in enumerate(model.lif_layers):\n",
    "    print(f\"Layer {i}: {lif.in_features} â {lif.out_features}\")\n",
    "\n",
    "for i, G in enumerate(model.G_hiddens):\n",
    "    print(f\"G[{i}]: {G.shape}\")\n",
    "\n",
    "for i, lif in enumerate(model.lif_layers):\n",
    "    if lif.W.grad is not None:\n",
    "        print(f\"Layer {i} grad mean: {lif.W.grad.abs().mean().item():.6f}\")\n",
    "        print(f\"Layer {i} grad max: {lif.W.grad.abs().max().item():.6f}\")\n",
    "\n",
    "\n",
    "print(f\"accuracy is --> \", acc)\n",
    "spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "for i in range(len(spikes_means)):\n",
    "    print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6c6bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1 | Batch 400/938 | Loss: 2.3035\n",
      "layer 1 spike means: 0.48603516817092896\n",
      "layer 2 spike means: 0.44023439288139343\n",
      "layer 3 spike means: 0.0\n",
      "Epoch 1 | Batch 800/938 | Loss: 2.2954\n",
      "layer 1 spike means: 0.5793213248252869\n",
      "layer 2 spike means: 0.4284912049770355\n",
      "layer 3 spike means: 0.019453125074505806\n",
      "Epoch 1 finished. Test accuracy: 9.90%\n",
      "Epoch 2 | Batch 400/938 | Loss: 2.2277\n",
      "layer 1 spike means: 0.5710693597793579\n",
      "layer 2 spike means: 0.43400880694389343\n",
      "layer 3 spike means: 0.009531250223517418\n",
      "Epoch 2 | Batch 800/938 | Loss: 2.2366\n",
      "layer 1 spike means: 0.5257934927940369\n",
      "layer 2 spike means: 0.46015626192092896\n",
      "layer 3 spike means: 0.012578125111758709\n",
      "Epoch 2 finished. Test accuracy: 9.80%\n",
      "Epoch 3 | Batch 400/938 | Loss: 2.2305\n",
      "layer 1 spike means: 0.53448486328125\n",
      "layer 2 spike means: 0.4595947265625\n",
      "layer 3 spike means: 0.0015624999068677425\n",
      "Epoch 3 | Batch 800/938 | Loss: 2.2329\n",
      "layer 1 spike means: 0.522143542766571\n",
      "layer 2 spike means: 0.469970703125\n",
      "layer 3 spike means: 0.01406249962747097\n",
      "Epoch 3 finished. Test accuracy: 9.80%\n",
      "Epoch 4 | Batch 400/938 | Loss: 2.2422\n",
      "layer 1 spike means: 0.515869140625\n",
      "layer 2 spike means: 0.47102051973342896\n",
      "layer 3 spike means: 0.0078125\n",
      "Epoch 4 | Batch 800/938 | Loss: 2.2417\n",
      "layer 1 spike means: 0.516223132610321\n",
      "layer 2 spike means: 0.47209474444389343\n",
      "layer 3 spike means: 0.0015624999068677425\n",
      "Epoch 4 finished. Test accuracy: 24.71%\n",
      "Epoch 5 | Batch 400/938 | Loss: 2.2359\n",
      "layer 1 spike means: 0.5170532464981079\n",
      "layer 2 spike means: 0.47407227754592896\n",
      "layer 3 spike means: 0.00624999962747097\n",
      "Epoch 5 | Batch 800/938 | Loss: 2.2382\n",
      "layer 1 spike means: 0.529345691204071\n",
      "layer 2 spike means: 0.4845214784145355\n",
      "layer 3 spike means: 0.009374999441206455\n",
      "Epoch 5 finished. Test accuracy: 9.80%\n",
      "accuracy is -->  9.8\n",
      "Layer 0 density ->  tensor(0.5297)\n",
      "Layer 1 density ->  tensor(0.4594)\n",
      "Layer 2 density ->  tensor(0.0082)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 5\n",
    "model, optimizer = get_model_eligibility([784, 64, 32, 10], thresholds = [3, 2, 0.09], lr = 0.05)#increase lrto increase convergence speed\n",
    "\n",
    "model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, apply_poisson=False)\n",
    "\n",
    "print(f\"accuracy is --> \", acc)\n",
    "spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "for i in range(len(spikes_means)):\n",
    "    print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "epoch = 5\n",
    "model, optimizer = get_model_eligibility([784, 256, 128, 10], thresholds = [3, 2, 0.09], lr = 0.05)#increase lrto increase convergence speed\n",
    "\n",
    "model, acc, spikes_means = train_eligibility(train_loader, test_loader, model, optimizer, num_epochs=epoch, verbose=True, apply_poisson=False)\n",
    "\n",
    "print(f\"accuracy is --> \", acc)\n",
    "spikes_means = torch.mean(spikes_means, dim = 0, keepdim=False)\n",
    "for i in range(len(spikes_means)):\n",
    "    print(f\"Layer {i} density -> \", spikes_means[i])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3295cbef",
   "metadata": {},
   "source": [
    "Overall, SNNs are a great way to reduce the energy needs of models in both training and test phases. However, there are lots of variety on how we construct our layers, for example which cell do we choose, or which surrogate gradients we should use etc etc.. and parameter tuning is an important thing in SNNs.\n",
    "\n",
    "For example in the https://www.pnas.org/doi/10.1073/pnas.2109194119 this article you can see the accuracy results of different implementations, ranging from\n",
    "71.2% to 98.1%. \n",
    "According to the article, they used augmentation, also they used BPTT for this results trained it for 100 epochs. I trained a model above up to 15 epochs, which was achieved 93% rate, if I tune the values even a little bit I might have approached that level too. In addition, that article used sparsity regularization, increases the sparsity finding the tradeoff between performance and sparsity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a6e77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
